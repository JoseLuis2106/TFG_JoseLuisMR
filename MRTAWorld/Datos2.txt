Nota: La media de recompensas de la prueba no tiene por qué ser representativa (solo son 10 épocas). Sin embargo, puede dar una idea de cómo de bien actúa cada versión (además de observar su comportamiento en simulación).
Nota: Version final: Sin tiempo en los estados, distancias en lugar de posiciones, no reasignar tareas ya asignadas, sin task allocation en los estados.
Nota: Las recompensas con bucle while pueden estar "infladas", ya que al eliminar el bucle y para que ambos comportamientos (con y sin renderizado) sean lo mas parecidos posibles, se suprimieron los movimientos diagonales que si tienen las pruebas con el bucle.


Normal (con bucle while, 3 tareas):
Media recompensas prueba: 154.4 (maximo "posible": 160)
Parece que asigna las tareas de forma bastante eficiente.
Tiempo de entrenamiento: 10173.6s (2h 49min 33s)
Nº máximo estados: ~2.6*10^5

5 tareas (con bucle while):
Media recompensas prueba: 162.0 (maximo "posible": 200)
Parece que asigna las tareas de forma algo eficiente (quizás necesite más épocas).
Tiempo de entrenamiento: 21767.1s (6h 2min 47s)
Nº máximo estados: ~1.1*10^9

Sin bucle while durante entrenamiento (3 tareas):
Media recompensas prueba: 154.5 (maximo "posible": 160)
Parece que asigna las tareas de forma bastante eficiente.
Tiempo de entrenamiento: 9295.8s (2h 34min 55s)
Nº máximo estados: ~2.6*10^5

Sin bucle while y 5 tareas:
Media recompensas prueba:  156.0 (maximo "posible": 200)
Parece que asigna las tareas de forma algo eficiente (quizás necesite más épocas).
Tiempo de entrenamiento: 20816.4s (5h 46min 56s)
Nº máximo estados: ~1.1*10^9

Sin bucle while, 5 tareas y 10M épocas:
Media recompensas prueba: 154.7 (maximo "posible": 200)
Parece que asigna las tareas de forma mas eficiente (¿aun más épocas?).
Tiempo de entrenamiento: 22956.2s (6h 22min 36s)
Nº máximo estados: ~1.1*10^9